# -*- coding: utf-8 -*-
"""FINANCIAL INCLUSION IN AFRICA 23-07-2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13xE1qlnVfzodFq-gKYF79JiMsR2tp9yc

**Explolatory Data Analysis For Financial inclusion in Africa Dataset**
**Understand The Problem Statement**
Financial Inclusion remains one of the main obstacles to economic and human development in Africa. For example, across Kenya, Rwanda, Tanzania, and Uganda only 9.1 million adults (or 13.9% of the adult population) have access to or use a commercial bank account.

Traditionally, access to bank accounts has been regarded as an indicator of financial inclusion. Despite the proliferation of mobile money in Africa, and the growth of innovative fintech solutions, banks still play a pivotal role in facilitating access to financial services. Access to bank accounts enable households to save and facilitate payments while also helping businesses build up their credit-worthiness and improve their access to other finance services. Therefore, access to bank accounts is an essential contributor to long-term economic growth.

The objective of this Dataset is to create a machine learning model to predict which individuals are most likely to have or use a bank account. The models and solutions developed can provide an indication of the state of financial inclusion in Kenya, Rwanda, Tanzania and Uganda, while providing insights into some of the key demographic factors that might drive individuals’ financial outcomes.

Data source available in the the zindi platform, Zindi Africa

**Type of the Problem**
It is a classification problem where we have to predict whether individuals are most likely to have or use a bank account or not.In a classification problem, we have to predict discrete values based on a given set of independent variable.

**Hypothesis Generation**
This is a very important stage in any data science/machine learning pipeline. It involves understanding the problem in detail by brainstorming as many factors as possible which can impact the outcome. It is done by understanding the problem statement thoroughly and before looking at the data.

Below are some of the factors which I think can affect the chance for a person to have a bank account

People who have mobile phone have lower chance to use bank account because of mobile money services.
People who are employed have a higher chance of having a bank account than People who are unemployed.
people with low education have low chance to have bank account
people in rural areas have low chance to have bank account
people who have age below 18 have low chance to have bank account
female have less chance to have bank account
"""

# from google.colab import drive
# drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# machine learning
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
plt.rcParams["axes.labelsize"] = 18
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

"""rcParams is used to set the default size of the labels on the axes in Matplotlib plots.

%: This indicates an IPython magic command.

matplotlib: This specifies the library for which the inline behavior is being set.

inline: This argument tells IPython to display the plots within the notebook itself, as opposed to opening them in a new window.
"""

train = pd.read_csv('/content/drive/MyDrive/inclusion-financiere-en-afrique/Train.csv')
test = pd.read_csv('/content/drive/MyDrive/inclusion-financiere-en-afrique/Test.csv')
ss = pd.read_csv('/content/drive/MyDrive/inclusion-financiere-en-afrique/SampleSubmission.csv')
variables = pd.read_csv('/content/drive/MyDrive/inclusion-financiere-en-afrique/VariableDefinitions.csv')

# Let’s observe the shape of our datasets.
print('train data shape :', train.shape)
print('test data shape :', test.shape)

"""The above output show the number of rows and columns for dataset (train and test)

# **Explolatory Data Analysis**
This is the process of finding some insights from you dataset before create predictive models.
"""

#show list of columns in train data
list(train.columns)

# inspect train data
train.head()

# Check for missing values
print('missing values:', train.isnull().sum())

# Explore Target distribution
sns.catplot(x="bank_account", kind="count", data=train, palette="Set1")

"""
The data shows that we have large number of no class than yes class in our target variable"""



# view the submission file
ss.head()

#show some information about the dataset
print(train.info())

"""The outshow shows the list of variables , sizes and data types in each variables. This will help you to know what feature engineering you can apply."""

# Let's view the variables
variables

"""# **A.Univariate Analysis**
In this section, we will do univariate analysis. It is the simplest form of analyzing data where we examine each variable individually. For categorical features we can use frequency table or bar plots which will calculate the number of each category in a particular variable. For numerical features, probability density plots can be used to look at the distribution of the variable.
"""

# Frequency table of a variable will give us the count of each category in that Target variable.
train['bank_account'].value_counts()

# Explore Target distribution

sns.catplot(x="bank_account", kind="count", data= train)

"""
The data shows that we have large number of no class than yes class in our target variable"""

# Explore Country distribution

sns.catplot(x="country", kind="count", data=train, palette="colorblind")

"""
Most of the data where collected in Rwanda and less data collected in Uganda"""

# Explore Location distribution
sns.catplot(x="location_type", kind="count", data=train, palette="colorblind")

"""
Most people live in rural area than urban area"""

# Explore year distribution
sns.catplot(x="year", kind="count", data=train, palette="colorblind")

"""Most of the data were collected in 2016"""

# Explore cellphone_access distribution
sns.catplot(x="cellphone_access", kind="count", data=train, palette="colorblind")

"""
Most of the particiapants have access to the cellphone"""

# Explore gender_of_respondent distribution
sns.catplot(x="gender_of_respondent", kind="count", data=train, palette="colorblind")

"""
We have more Females than Males"""

# Explore relationship_with_head distribution

sns.catplot(x="relationship_with_head", kind="count", data=train, palette="colorblind");

plt.xticks(
    rotation=45,
    horizontalalignment='right',
    fontweight='light',
    fontsize='x-large'
)

"""
We have more head of Household particants and few other non-relatives"""

# Explore marital_status distribution

sns.catplot(x="marital_status", kind="count", data=train, palette="colorblind");

plt.xticks(
    rotation=45,
    horizontalalignment='right',
    fontweight='light',
    fontsize='x-large'
)

"""
Most of the participants are maried/living together"""

# Explore education_level distribution

sns.catplot(x="education_level", kind="count", data=train, palette="colorblind");

plt.xticks(
    rotation=45,
    horizontalalignment='right',
    fontweight='light',
    fontsize='x-large'
)

"""
Most of the participants have primary education level"""

# Explore job_type distribution

sns.catplot(x="job_type", kind="count", data=train, palette="colorblind");

plt.xticks(
    rotation=45,
    horizontalalignment='right',
    fontweight='light',
    fontsize='x-large'
)

"""Most of the participants are self employed"""

# Explore household_size distribution

plt.figure(figsize=(16, 6))
train.household_size.hist()
plt.xlabel('Household  size')

# Get the unique values of household_size
unique_household_sizes = train.household_size.unique()
print("Unique household sizes:", unique_household_sizes)

# Get the count of each unique household size
household_size_counts = train.household_size.value_counts()

# Find the most common household size and its count
most_common_household_size = household_size_counts.idxmax()
print("Most common number of people in household:", most_common_household_size)

most_common_household_size_count = household_size_counts.max()
print("Count of the most common household size:", most_common_household_size_count)

"""
Household_size is not normally distributed and the most common number of people living in the house is 2"""

# Explore age_of_respondent distribution
plt.figure(figsize=(16, 6))
train.age_of_respondent.hist()
plt.xlabel('Age of Respondent')

"""most of the participant's age is between 20's and 40

# **B.Bivariate Analysis**
Bivariate analysis is the simultaneous analysis of two variables (attributes). It explores the concept of relationship between two variables, whether there exists an association and the strength of this association, or whether there are differences between two variables and the significance of these differences.

After looking at every variable individually in univariate analysis, we will now explore them again with respect to the target variable.
"""

plt.figure(figsize=(16, 6))
sns.catplot(x='location_type', hue='bank_account', kind='count', data=train)
plt.xticks(
    fontweight='light',
    fontsize='x-large'
)

plt.figure(figsize=(16, 6))
sns.catplot(x='gender_of_respondent', hue='bank_account', kind='count', data=train)
plt.xticks(
    rotation=45,
    horizontalalignment='right',
    fontweight='light',
    fontsize='x-large'
)

plt.figure(figsize=(16, 6))
sns.catplot(x='cellphone_access', hue='bank_account', kind='count', data=train)
plt.xticks(
    rotation=45,
    horizontalalignment='right',
    fontweight='light',
    fontsize='x-large'
)

plt.figure(figsize=(16, 6))
sns.catplot(x='relationship_with_head', hue='bank_account', kind='count', data=train)
plt.xticks(
    rotation=45,
    horizontalalignment='right',
    fontweight='light',
    fontsize='x-large'
)

plt.figure(figsize=(16, 6))
sns.catplot(x='marital_status', hue='bank_account', kind='count', data=train)
plt.xticks(
    rotation=45,
    horizontalalignment='right',
    fontweight='light',
    fontsize='x-large'
)

plt.figure(figsize=(16, 6))
sns.catplot(x='education_level', hue='bank_account', kind='count', data=train)
plt.xticks(
    rotation=45,
    horizontalalignment='right',
    fontweight='light',
    fontsize='x-large'
)

plt.figure(figsize=(16, 6))
sns.catplot(x='job_type', hue='bank_account', kind='count', data=train)
plt.xticks(
    rotation=45,
    horizontalalignment='right',
    fontweight='light',
    fontsize='x-large'
)

#import preprocessing module
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler

# Cobvert target label to numerical Data
le = LabelEncoder()
train['bank_account'] = le.fit_transform(train['bank_account'])

#Separate training features from target
X_train = train.drop(['bank_account'], axis=1)
y_train = train['bank_account']

print(y_train)

"""# **Our Hypothesis Results**

People who have mobile phone have lower chance to use bank account because of mobile money services.-TRUE

People who are employed have a higher chance of having a bank account than People who are unemployed. -TRUE

people with low education have low chance to have bank account - TRUE

people in rural areas have low chance to have bank account -TRUE

people who have age below 18 have low chance to have bank account-TRUE

female have less chance to have bank account -TRUE

# **DATA PREPROCESSING**

When using scaling methods like MinMaxScaler or StandardScaler, it’s generally a good practice to work with floating-point numbers:

Precision: Scaling operations require precision that is naturally handled by floating-point arithmetic. Integer operations might lead to rounding issues or loss of precision.

Library Expectations: Some libraries expect numerical inputs in float format to avoid type-related issues.
"""

# function to preprocess our data from train models
def preprocessing_data(data):

    # Convert the following numerical labels from interger to float
    float_array = data[["household_size", "age_of_respondent", "year"]].values.astype(float)

    # categorical features to be onverted to One Hot Encoding
    categ = ["relationship_with_head", "marital_status", "education_level", "job_type", "country"]

    # One Hot Encoding conversion
    data = pd.get_dummies(data, prefix_sep="_", columns=categ)

    # Label Encoder conversion
    data["location_type"] = le.fit_transform(data["location_type"])
    data["cellphone_access"] = le.fit_transform(data["cellphone_access"])
    data["gender_of_respondent"] = le.fit_transform(data["gender_of_respondent"])

    # drop uniquid column
    data = data.drop(["uniqueid"], axis=1)

    # scale our data into range of 0 and 1
    scaler = MinMaxScaler(feature_range=(0, 1))
    data = scaler.fit_transform(data)

    return data

# preprocess the train data
processed_train = preprocessing_data(X_train)
processed_test = preprocessing_data(test)

# view the first row of the processed_train dataset after preprocessing.
#Inclusive of Start, Exclusive of End
print(processed_train[:2])

# shape of the processed train set
print(processed_train.shape)

import sklearn.model_selection

# Split train_data
from sklearn.model_selection import train_test_split

X_Train, X_Val, y_Train, y_val = train_test_split(processed_train, y_train, stratify = y_train,
                                                  test_size =28, random_state=42)

#import classifier algorithm here
from xgboost import XGBClassifier

# create models
xg_model = XGBClassifier()

#fitting the models
xg_model.fit(X_Train,y_Train)

# import evaluation metrics
from sklearn.metrics import confusion_matrix, accuracy_score

# evaluate the model
xg_y_model = xg_model.predict(X_Val)

# Get error rate
print("Error rate of XGB classifier: ", 1 - accuracy_score(y_val, xg_y_model))

#print the classification report
from sklearn.metrics import classification_report

report = classification_report(y_val, xg_y_model)
print(report)

# calculate the accuracy and prediction of the model
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
xgboost_model_predicted = xg_model.predict(X_Val)
score = accuracy_score(y_val, xgboost_model_predicted)
print("Error rate for XGBClassifie model is: ", 1- score)
# Calculate confusion matrix
cm = confusion_matrix(y_val, xgboost_model_predicted, normalize='true')
print("Confusion Matrix:")
print(cm)

# Plot confusion matrix as a heatmap
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_val))
disp.plot(cmap='viridis', values_format='.2f')
plt.title("Confusion Matrix")
plt.show()

# Get the predicted result for the test Data
test.bank_account = xg_model.predict(processed_test)

# Create submission DataFrame
submission = pd.DataFrame({"uniqueid": test["uniqueid"] + " x " + test["country"],
                           "bank_account": test.bank_account})

#show the five sample
submission.sample(5)

# Create submission csv file csv file
from google.colab import files
submission.to_csv('2_1 submission.csv', index = False)
files.download('2_1 submission.csv')

